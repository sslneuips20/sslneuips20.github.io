
<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Schedule | NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice
</title>
  <link rel="canonical" href="../pages/Accepted Paper.html">



  <link rel="stylesheet" href="../theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="../theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="../theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="../theme/css/style.css">
  <link rel="stylesheet" href="../theme/css/custom.css">


<meta name="description" content="The workshop will be held December 11 or 12 virtually. Tenative schedule is below with possible changes to accommodate virtual format better. Time zone TBD. Videos will be posted here after the conference. Morning Program: 09:00 - 13:00 09:00 - 09:10 Opening remarks 09:10 - 09:40 Keynote â€¦">
</head>

<body>
  <header class="header">
    <div class="container">
      <div class="row">
        <!-- <div class="col-xs-2">
          <a href="../">
            <img class="img-fluid" src=../images/logo.png alt="ML4H: Machine Learning for Health">
          </a>
        </div> -->
        <div class="col-xs-10">
          <h1 class="title" style="width:950px"><a href="../">NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice</a></h1>
          <!-- <p class="text-muted">Workshop at NeurIPS 2020</p> -->
          <br />
          <ul class="list-inline" style="width:max-content"> 
            <li class="list-inline-item"><a href="../index.html">Home</a></li>
            <li class="list-inline-item"><a href="../pages/call-for-participation.html">Call for Submissions</a></li>
            <li class="list-inline-item"><a href="../pages/schedule.html">Schedule</a></li>
            <li class="list-inline-item"><a href="../pages/speakers.html">Speakers</a></li>
            <li class="list-inline-item"><a href="../pages/organizers.html">Organizers</a></li>
            <li class="list-inline-item"><a href="../pages/Program Committee.html">Program Committee</a></li>
            <li class="list-inline-item"><a href="../pages/Accepted Paper.html">Accepted Paper</a></li>
          </ul>
        </div>
      </div>
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>Accepted Abstracts
</h1>
      <hr>
<article class="article">
  <div class="content" id="test" style="width:1050px;">

<!-- <tr align="center"> <td>Zihang	Lai</td> <td>University of Oxford</td></tr> -->






</div>

<!-- <h3> Lunch/dinner break: 13:00 - 14:00 </h3> -->

<!-- <h3>Afternoon Program: 13:30 - 18:25</h3> -->

<!-- <div class="table-responsive">
  <table class="table table-bordered">
    <tbody>
<tr> <td>13:30 - 13:55</td> <td>Invited Talk 7</td></tr>
<tr> <td>13:55 - 14:20</td> <td>Invited Talk 8</td></tr>
<tr> <td>14:20 - 14:45</td> <td>Invited Talk 9</td></tr>
<tr> <td>14:45 - 15:10</td> <td>Invited Talk 10</td></tr>
<tr> <td>15:10 - 16:10</td> <td>Poster Session II</td></tr>
<tr> <td>16:10 - 16:35</td> <td>Invited Talk 11</td></tr>
<tr> <td>16:35 - 17:00</td> <td>Invited Talk 12</td></tr>
<tr> <td>17:00 - 17:20</td> <td>Invited Talk 13</td></tr>
<tr> <td>17:25 - 18:25</td> <td>Panel Discussion & Closing</td></tr>
    </tbody>
    </table>
</div> -->



  </div>
</article>
    </div>
  </div>

<script>
    document.getElementById("test").innerHTML = "<li><b><u>Self-Supervised Learning for Large-Scale Unsupervised Image Clustering</u></b> - <i>Zheltonozhskii, Baskin, Bronstein, Mendelson</i></li><li><b><u>Learning Self-Expression Metrics for Scalable and Inductive Subspace Clustering</u></b> - <i>Busch, Faerman, Schubert, Seidl</i></li><li><b><u>Self-alignment Pre-training for Biomedical Entity Representations</u></b> - <i>Liu, Shareghi, Meng, Basaldella, Collier</i></li><li><b><u>Prototypical Contrastive Learning of Unsupervised Representations</u></b> - <i>Li, Zhou, Xiong, Hoi</i></li><li><b><u>i-Mix: A Strategy for Regularizing Contrastive Representation Learning</u></b> - <i>Lee, Zhu, Sohn, Li, Shin, Lee</i></li><li><b><u>MixCo: Mix-up Contrastive Learning for Visual Representation</u></b> - <i>Lee, Kim, Bae, Yun</i></li><li><b><u>Self-supervised Representation Learning with Relative Predictive Coding</u></b> - <i>Tsai, Ma, Yang, Zhao, Salakhutdinov, Morency</i></li><li><b><u>A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks</u></b> - <i>Saunshi, Malladi, Arora</i></li><li><b><u>SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning</u></b> - <i>Kim, Choo, Kwon, Joe, Min, Gwon</i></li><li><b><u>Spatiotemporal Contrastive Video Representation Learning</u></b> - <i>Qian, Meng, Gong, Yang, Wang, Belongie, Cui</i></li><li><b><u>Make Lead Bias in Your Favor: Zero-shot Abstractive News Summarization</u></b> - <i>Zhu, Yang, Gmyr, Zeng, Huang</i></li><li><b><u>Semantic Augmentation with Self-Supervised Content Mixing for Semi-Supervised Learning</u></b> - <i>Sun, Masson, Henaff, Thome, Cord</i></li><li><b><u>Self-Supervised Object-Wise 3D Decomposition of Images using Shape Priors</u></b> - <i>Elich, Oswald, Pollefeys, Stueckler</i></li><li><b><u>Contrastive Learning with Hard Negative Samples</u></b> - <i>Robinson, Chuang, Sra, Jegelka</i></li><li><b><u>BYOL works even without batch statistics</u></b> - <i>Richemond, Grill, Altch, Tallec, Strub, Piot, Valko</i></li><li><b><u>Self-supervised Learning from a Multi-view Perspective</u></b> - <i>Tsai, Wu, Salakhutdinov, Morency</i></li><li><b><u>Refining Pre-trained NLP Models Through Shuffled-token Detection</u></b> - <i>Panda, Agrawal, Ha, Bloch, Bowman</i></li><li><b><u>Understanding Self-supervised Learning with Dual Deep Networks</u></b> - <i>Tian, Yu, Chen, Ganguli</i></li><li><b><u>Visual Question Answering with Annotation-Efficient Zero Shot Learning under Linguistic Domain Shift</u></b> - <i>Banerjee, Gokhale, Yang, Baral</i></li><li><b><u>Data Transformation Insights in Self-Supervision with Clustering Tasks</u></b> - <i>Kumar, Deshmukh, Dogan, Charles, Manavoglu</i></li><li><b><u>Functional Regularization for Representation Learning: A Unified Theoretical Perspective</u></b> - <i>Garg, Liang</i></li><li><b><u>Cross-Domain Sentiment Classification With In-domain Contrastive Learning</u></b> - <i>Li, Chen, Dong, Zhang, Keutzer</i></li><li><b><u>Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency</u></b> - <i>Zhang, Xiao, Efros, Pinto, Wang</i></li><li><b><u>Pretraining Neural Architecture Search Controllers with Locality-based Self-Supervised Learning</u></b> - <i>Choi, Choe, Lee</i></li><li><b><u>Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision</u></b> - <i>Zhong, Miao, Xie, Wang</i></li><li><b><u>Representation Learning via Invariant Causal Mechanisms</u></b> - <i>Mitrovic, McWilliams, Walker, Buesing, Blundell</i></li><li><b><u>SSD: A Unified Framework for Self-Supervised Outlier Detection</u></b> - <i>Sehwag, Chiang, Mittal</i></li><li><b><u>Is It a Plausible Colour? UCapsNet for Colourisation</u></b> - <i>Pucci, MICHELONI, Foresti, Martinel</i></li><li><b><u>Self-guided Approximate Linear Programs</u></b> - <i>Pakiman, Nadarajah, Azad, Lin</i></li><li><b><u>SemanticCMC - improved semantic self-supervised learning with naturalistic temporal co-occurrences</u></b> - <i>O'Doherty, Cusack</i></li><li><b><u>Social NCE: Contrastive Learning for Socially-aware Trajectory Forecasting and Motion Planning</u></b> - <i>Liu, Yan, Alahi</i></li><li><b><u>Self-Supervised Ranking for Representation Learning</u></b> - <i>Varamesh, Diba, Tuytelaars, Gool</i></li><li><b><u>Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data</u></b> - <i>Wei, Shen, Chen, Ma</i></li><li><b><u>Contrast and Classify: Alternate Training for Robust VQA</u></b> - <i>Kant, Moudgil, Batra, Parikh, Agrawal</i></li><li><b><u>Predicting What You Already Know Helps: Provable Self-Supervised Learning</u></b> - <i>Lee, Lei, Saunshi, Zhuo</i></li><li><b><u>Environment Predictive Coding for Embodied Agents</u></b> - <i>Ramakrishnan, Nagarajan, Al-Halah, Grauman</i></li><li><b><u>Supervision Accelerates Pre-training in Contrastive Semi-Supervised Learning of Visual Representations</u></b> - <i>Assran, Ballas, Castrejon, Rabbat</i></li><li><b><u>Understanding self-supervised learning using controlled datasets with known structure</u></b> - <i>Hermann, Chen, Norouzi, Kornblith</i></li><li><b><u>Greedy Hierarchical Variational Autoencoders for Scalable Visual Dynamics Models</u></b> - <i>Wu, Nair, Martin-Martin, Finn, Fei-Fei</i></li><li><b><u>Pre-training Text-to-Text Transformers to Write and Reason with Concepts</u></b> - <i>Zhou, Lee, Selvam, Lee, Lin, Ren</i></li><li><b><u>Contrastive Learning can Identify the Underlying Generative Factors of the Data</u></b> - <i>Zimmermann, Schneider, Sharma, Bethge, Brendel</i></li><li><b><u>Evaluation of Out-of-Distribution Detection Performance of Self-Supervised Learning in a Controllable Environment</u></b> - <i>Park, Jo, Gwak, Hong, Choo, Choi</i></li><li><b><u>Hard Negative Mixing for Contrastive Learning</u></b> - <i>Kalantidis, , Pion, Weinzaepfel, Larlus</i></li><li><b><u>Generalized Adversarially Learned Inference</u></b> - <i>Dandi, Bharadhwaj, Kumar, Rai</i></li>";
</script>


</body>

</html>
